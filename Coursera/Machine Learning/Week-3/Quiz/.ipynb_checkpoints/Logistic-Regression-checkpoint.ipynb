{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  Suppose that you have trained a logistic regression classifier, and it outputs on a new example x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ans: \n",
    "- Our estimate for P(y=1|x;θ) is 0.2. \n",
    "- Our estimate for P(y=0|x;θ) is 0.8. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Suppose you have the following training set, and fit a logistic regression classifier ```h_\\theta(x) = g(\\theta_0 + \\theta_1x_1 + \\theta_2 x_2)```.\n",
    "![alt text](./data.png)\n",
    "![alt text](./plot.png)\n",
    "#### Which of the following are true? Check all that apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ans: \n",
    "- J(θ) will be a convex function, so gradient descent should converge to the global minimum. \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. For logistic regression, the gradient is given by ```∂∂/θj{J(θ)} = 1/m ∑i=1m(hθ(x(i))−y(i))xj(i)```. Which of these is a correct gradient descent update for logistic regression with a learning rate of ```α```? Check all that apply."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ans: ![image.png](./3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.  Which of the following statements are true? Check all that apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ans: \n",
    "- The cost function J(θ)J(\\theta)J(θ) for logistic regression trained with m≥1m \\geq 1m≥1 examples is always greater than or equal to zero. \n",
    "-  The one-vs-all technique allows you to use logistic regression for problems in which each y(i)y^{(i)}y(i) comes from a fixed, discrete set of values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Suppose you train a logistic classifier ```h_\\theta(x) = g(\\theta_0 + \\theta_1x_1 + \\theta_2 x_2)```. Suppose ```\\theta_0 = 6, \\theta_1 = 0, \\theta_2 = -1```. Which of the following figures represents the decision boundary found by your classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ans: ![image.png](./5.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
